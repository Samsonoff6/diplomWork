{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Flatten\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, concatenate, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Открываем наш Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train = pd.read_csv(\"C:/Users/samso/Desktop/all/train.csv\",encoding=\"latin-1\",error_bad_lines=False)\n",
    "train=pd.read_csv(\"train_2.csv\")\n",
    "#train=train.drop([\"ItemID\"],axis=1)\n",
    "train=train.drop([\"Unnamed: 0\"],axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Производим обработку Dataset`a (удаление ненужных символов , приводим к нижнему регистру)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          is so sad for my apl friend\n",
       "1                        i missed the new moon trailer\n",
       "2                                omg its already 730 o\n",
       "3               omgaga im sooo  im gunna cry ive be...\n",
       "4             i think mi bf is cheating on me      ...\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"SentimentText\"].replace('', np.nan, inplace=True)\n",
    "train.dropna(inplace=True)\n",
    "\n",
    "alpha=train.loc[train['SentimentText'].apply(lambda x:\"@\" in x.lower()),:]\n",
    "\n",
    "train[\"SentimentText\"]=train[\"SentimentText\"].replace(\"[^\\w ]+\",\"\",regex=True)\n",
    "train[\"SentimentText\"]=train[\"SentimentText\"].apply(lambda x:x.lower())\n",
    "\n",
    "\n",
    "train[\"SentimentText\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 730 o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo  im gunna cry ive be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>juuuuuuuuuuuuuuuuussssst chillin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>sunny again        work tomorrow       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today  i miss you a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm i wonder how she my number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>i must think about positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>thanks to all the haters up in my face a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>this weekend has sucked so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>jb isnt showing in australia any more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>ok thats it you win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>lt this is the way i feel right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>awhhe man im completely useless rt now fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>feeling strangely fine now im gonna go lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>huge roll of thunder just nowso scary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>i just cut my beard off its only been grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad about iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>wompppp wompp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>youre the only one who can see this cause ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>ltsad level is 3 i was writing a massive bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>headed to hospitol  had to pull out of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>boring    whats wrong with him     please t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>cant be bothered i wish i could spend the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>feeeling like shit right now i really want ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>goodbye exams hello alcohol tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>i didnt realize it was that deep geez give ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99958</th>\n",
       "      <td>0</td>\n",
       "      <td>will you guys be swinging through sf anytime s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99959</th>\n",
       "      <td>0</td>\n",
       "      <td>it made me sad too  that means no more albums</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99960</th>\n",
       "      <td>1</td>\n",
       "      <td>i agree i think they all have that fetish too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99961</th>\n",
       "      <td>0</td>\n",
       "      <td>i hope its not too serious of an injury  im wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99962</th>\n",
       "      <td>0</td>\n",
       "      <td>if its any consolation this weekend isnt quite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99963</th>\n",
       "      <td>1</td>\n",
       "      <td>got your back yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99964</th>\n",
       "      <td>1</td>\n",
       "      <td>i cant wait to see that movie enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99965</th>\n",
       "      <td>1</td>\n",
       "      <td>i am excited and a little nervous i cant wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99966</th>\n",
       "      <td>0</td>\n",
       "      <td>yeah  sorrygoing to a concert that nightnon re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99967</th>\n",
       "      <td>1</td>\n",
       "      <td>thanks for spreading the word for diabetes in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99968</th>\n",
       "      <td>1</td>\n",
       "      <td>im so glad you went to china town again  i act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99969</th>\n",
       "      <td>0</td>\n",
       "      <td>sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>0</td>\n",
       "      <td>damn it dont have sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>0</td>\n",
       "      <td>thats the thing the new raft of star wars film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>1</td>\n",
       "      <td>followfriday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>0</td>\n",
       "      <td>awaresg you are not wrong but from a my own ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>0</td>\n",
       "      <td>cuz you big burly man  hahahahahahahahaha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>1</td>\n",
       "      <td>trying to get a wider range of shirts to suit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>1</td>\n",
       "      <td>haha i love the passion in your support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>1</td>\n",
       "      <td>that sucksi like living in coopersville i dont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>1</td>\n",
       "      <td>till i can go home been here till saturday  x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>1</td>\n",
       "      <td>afternoon jim hows you  x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>0</td>\n",
       "      <td>the foot is really bad like the worst its ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>1</td>\n",
       "      <td>have fun doing health amp safety s just switch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>0</td>\n",
       "      <td>it took me waaay too long to get your message ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>0</td>\n",
       "      <td>seems like a repeating problem   hope youre a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>1</td>\n",
       "      <td>arrrr we both replied to each other over diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>0</td>\n",
       "      <td>ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>1</td>\n",
       "      <td>yes yes im glad you had more fun with me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>1</td>\n",
       "      <td>haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99712 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                      SentimentText\n",
       "0              0                        is so sad for my apl friend\n",
       "1              0                      i missed the new moon trailer\n",
       "2              1                              omg its already 730 o\n",
       "3              0             omgaga im sooo  im gunna cry ive be...\n",
       "4              0           i think mi bf is cheating on me      ...\n",
       "5              0                   or i just worry too much        \n",
       "6              1                   juuuuuuuuuuuuuuuuussssst chillin\n",
       "7              0         sunny again        work tomorrow       ...\n",
       "8              1        handed in my uniform today  i miss you a...\n",
       "9              1                  hmmmm i wonder how she my number \n",
       "10             0                        i must think about positive\n",
       "11             1        thanks to all the haters up in my face a...\n",
       "12             0                     this weekend has sucked so far\n",
       "13             0              jb isnt showing in australia any more\n",
       "14             0                                ok thats it you win\n",
       "15             0                lt this is the way i feel right now\n",
       "16             0      awhhe man im completely useless rt now fun...\n",
       "17             1      feeling strangely fine now im gonna go lis...\n",
       "18             0              huge roll of thunder just nowso scary\n",
       "19             0      i just cut my beard off its only been grow...\n",
       "20             0                                very sad about iran\n",
       "21             0                                      wompppp wompp\n",
       "22             1      youre the only one who can see this cause ...\n",
       "23             0     ltsad level is 3 i was writing a massive bl...\n",
       "24             0       headed to hospitol  had to pull out of th...\n",
       "25             0     boring    whats wrong with him     please t...\n",
       "26             0     cant be bothered i wish i could spend the r...\n",
       "27             0     feeeling like shit right now i really want ...\n",
       "28             1               goodbye exams hello alcohol tonight \n",
       "29             0     i didnt realize it was that deep geez give ...\n",
       "...          ...                                                ...\n",
       "99958          0  will you guys be swinging through sf anytime s...\n",
       "99959          0     it made me sad too  that means no more albums \n",
       "99960          1     i agree i think they all have that fetish too \n",
       "99961          0  i hope its not too serious of an injury  im wo...\n",
       "99962          0  if its any consolation this weekend isnt quite...\n",
       "99963          1                                  got your back yo \n",
       "99964          1               i cant wait to see that movie enjoy \n",
       "99965          1     i am excited and a little nervous i cant wait \n",
       "99966          0  yeah  sorrygoing to a concert that nightnon re...\n",
       "99967          1   thanks for spreading the word for diabetes in...\n",
       "99968          1  im so glad you went to china town again  i act...\n",
       "99969          0                                              sorry\n",
       "99970          0                             damn it dont have sky \n",
       "99971          0  thats the thing the new raft of star wars film...\n",
       "99973          1                                       followfriday\n",
       "99974          0  awaresg you are not wrong but from a my own ma...\n",
       "99975          0          cuz you big burly man  hahahahahahahahaha\n",
       "99976          1  trying to get a wider range of shirts to suit ...\n",
       "99977          1           haha i love the passion in your support \n",
       "99978          1  that sucksi like living in coopersville i dont...\n",
       "99979          1      till i can go home been here till saturday  x\n",
       "99980          1                          afternoon jim hows you  x\n",
       "99981          0  the foot is really bad like the worst its ever...\n",
       "99982          1  have fun doing health amp safety s just switch...\n",
       "99983          0  it took me waaay too long to get your message ...\n",
       "99984          0   seems like a repeating problem   hope youre a...\n",
       "99985          1  arrrr we both replied to each other over diffe...\n",
       "99986          0                                   ya i thought so \n",
       "99987          1          yes yes im glad you had more fun with me \n",
       "99988          1                                   haha yes you do \n",
       "\n",
       "[99712 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(data):\n",
    "    for i in tqdm(range((data.shape[0]))):\n",
    "        data.loc[i,\"SentimentText\"]=re.sub(\"@ ?.+? \",\"\",data.loc[i,\"SentimentText\"],re.DOTALL)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 99989/99989 [22:05<00:00, 75.46it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha=cleaner(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.to_csv(\"train_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем тестовую и обучающую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=train.loc[0:83325]#основная\n",
    "data_test=train.loc[83326:99989]#тестовая выборка\n",
    "\n",
    "X_train=data_train[\"SentimentText\"]\n",
    "y_train=data_train[\"Sentiment\"]\n",
    "\n",
    "X_test=data_test[\"SentimentText\"]\n",
    "y_test=data_test[\"Sentiment\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train[\"SentimentText\"]\n",
    "text.to_csv(\"C:/Users/samso/Desktop/diplom data/all/tweet.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью библиотеки word2vec задаем словам вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-01 15:20:50,286 : INFO : collecting all words and their counts\n",
      "2018-11-01 15:20:50,286 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-01 15:20:50,395 : INFO : PROGRESS: at sentence #10000, processed 127663 words, keeping 28904 word types\n",
      "2018-11-01 15:20:50,489 : INFO : PROGRESS: at sentence #20000, processed 263568 words, keeping 50920 word types\n",
      "2018-11-01 15:20:50,629 : INFO : PROGRESS: at sentence #30000, processed 395126 words, keeping 68007 word types\n",
      "2018-11-01 15:20:50,754 : INFO : PROGRESS: at sentence #40000, processed 528098 words, keeping 84399 word types\n",
      "2018-11-01 15:20:50,895 : INFO : PROGRESS: at sentence #50000, processed 659642 words, keeping 100143 word types\n",
      "2018-11-01 15:20:51,067 : INFO : PROGRESS: at sentence #60000, processed 791837 words, keeping 115577 word types\n",
      "2018-11-01 15:20:51,192 : INFO : PROGRESS: at sentence #70000, processed 925505 words, keeping 130782 word types\n",
      "2018-11-01 15:20:51,317 : INFO : PROGRESS: at sentence #80000, processed 1057849 words, keeping 145730 word types\n",
      "2018-11-01 15:20:51,473 : INFO : PROGRESS: at sentence #90000, processed 1188067 words, keeping 160543 word types\n",
      "2018-11-01 15:20:51,660 : INFO : collected 175160 word types from a corpus of 1319725 raw words and 99989 sentences\n",
      "2018-11-01 15:20:51,660 : INFO : Loading a fresh vocabulary\n",
      "2018-11-01 15:20:51,817 : INFO : min_count=3 retains 15669 unique words (8% of original 175160, drops 159491)\n",
      "2018-11-01 15:20:51,817 : INFO : min_count=3 leaves 1152641 word corpus (87% of original 1319725, drops 167084)\n",
      "2018-11-01 15:20:51,910 : INFO : deleting the raw counts dictionary of 175160 items\n",
      "2018-11-01 15:20:51,926 : INFO : sample=0.001 downsamples 58 most-common words\n",
      "2018-11-01 15:20:51,926 : INFO : downsampling leaves estimated 884244 word corpus (76.7% of prior 1152641)\n",
      "2018-11-01 15:20:52,098 : INFO : estimated required memory for 15669 words and 200 dimensions: 32904900 bytes\n",
      "2018-11-01 15:20:52,098 : INFO : resetting layer weights\n",
      "2018-11-01 15:20:52,567 : INFO : training model with 4 workers on 15669 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-01 15:20:53,582 : INFO : EPOCH 1 - PROGRESS: at 39.39% examples, 344924 words/s, in_qsize 0, out_qsize 0\n",
      "2018-11-01 15:20:54,582 : INFO : EPOCH 1 - PROGRESS: at 77.83% examples, 342648 words/s, in_qsize 0, out_qsize 0\n",
      "2018-11-01 15:20:55,066 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-01 15:20:55,082 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-01 15:20:55,082 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-01 15:20:55,098 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-01 15:20:55,098 : INFO : EPOCH - 1 : training on 1319725 raw words (883995 effective words) took 2.5s, 349578 effective words/s\n",
      "2018-11-01 15:20:56,160 : INFO : EPOCH 2 - PROGRESS: at 38.65% examples, 337397 words/s, in_qsize 0, out_qsize 0\n",
      "2018-11-01 15:20:57,176 : INFO : EPOCH 2 - PROGRESS: at 79.37% examples, 349725 words/s, in_qsize 0, out_qsize 1\n",
      "2018-11-01 15:20:57,598 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-01 15:20:57,613 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-01 15:20:57,629 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-01 15:20:57,629 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-01 15:20:57,629 : INFO : EPOCH - 2 : training on 1319725 raw words (884071 effective words) took 2.5s, 357743 effective words/s\n",
      "2018-11-01 15:20:58,676 : INFO : EPOCH 3 - PROGRESS: at 45.45% examples, 384749 words/s, in_qsize 0, out_qsize 1\n",
      "2018-11-01 15:20:59,691 : INFO : EPOCH 3 - PROGRESS: at 96.92% examples, 417461 words/s, in_qsize 0, out_qsize 0\n",
      "2018-11-01 15:20:59,722 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-01 15:20:59,722 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-01 15:20:59,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-01 15:20:59,754 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-01 15:20:59,754 : INFO : EPOCH - 3 : training on 1319725 raw words (883771 effective words) took 2.1s, 417103 effective words/s\n",
      "2018-11-01 15:21:00,785 : INFO : EPOCH 4 - PROGRESS: at 47.72% examples, 415816 words/s, in_qsize 0, out_qsize 0\n",
      "2018-11-01 15:21:01,722 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-01 15:21:01,738 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-01 15:21:01,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-01 15:21:01,754 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-01 15:21:01,754 : INFO : EPOCH - 4 : training on 1319725 raw words (884064 effective words) took 2.0s, 446141 effective words/s\n",
      "2018-11-01 15:21:02,769 : INFO : EPOCH 5 - PROGRESS: at 50.00% examples, 433539 words/s, in_qsize 0, out_qsize 0\n",
      "2018-11-01 15:21:03,691 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-11-01 15:21:03,691 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-01 15:21:03,706 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-01 15:21:03,722 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-01 15:21:03,722 : INFO : EPOCH - 5 : training on 1319725 raw words (884037 effective words) took 2.0s, 449696 effective words/s\n",
      "2018-11-01 15:21:03,722 : INFO : training on a 6598625 raw words (4419938 effective words) took 11.2s, 395881 effective words/s\n",
      "2018-11-01 15:21:03,738 : INFO : saving Word2Vec object under C:/Users/samso/Desktop/diplom data/all/model.w2v, separately None\n",
      "2018-11-01 15:21:03,738 : INFO : not storing attribute vectors_norm\n",
      "2018-11-01 15:21:03,738 : INFO : not storing attribute cum_table\n",
      "2018-11-01 15:21:04,472 : INFO : saved C:/Users/samso/Desktop/diplom data/all/model.w2v\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "data = gensim.models.word2vec.LineSentence(\"C:/Users/samso/Desktop/diplom data/all/tweet.txt\")\n",
    "model = Word2Vec(data, size=200, window=5, min_count=3, workers=multiprocessing.cpu_count())\n",
    "model.save(\"C:/Users/samso/Desktop/diplom data/all/model.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('raining', 0.8403500914573669),\n",
       " ('sunny', 0.8257066011428833),\n",
       " ('rainy', 0.822749137878418),\n",
       " ('outside', 0.7888936996459961),\n",
       " ('pouring', 0.7773435711860657),\n",
       " ('stormy', 0.7627218961715698),\n",
       " ('hot', 0.7615020871162415),\n",
       " ('cloudy', 0.7547500133514404),\n",
       " ('relaxing', 0.7543225884437561),\n",
       " ('sleeping', 0.7533775568008423)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1=\"cool\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем словарь с помощью токинезатора. Мы обрабатываем наш текст , создавая матрицу векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LENGTH = 30 # Высота матрицы (максимальное количество слов в коментарии)\n",
    "# Размер словаря\n",
    "NUM = 100000\n",
    "def get_sequences(tokenizer, x):\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    return pad_sequences(sequences, maxlen=SENTENCE_LENGTH)\n",
    "\n",
    "# Cоздаем и обучаем токенизатор\n",
    "tokenizer = Tokenizer(num_words=NUM)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Отображаем каждый текст в массив идентификаторов токенов\n",
    "x_train_seq = get_sequences(tokenizer, X_train)\n",
    "x_test_seq = get_sequences(tokenizer, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наш словарь слов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.15646613,  0.15922284,  0.29988083, ..., -0.15861644,\n",
       "         0.33668721, -1.15511692],\n",
       "       [ 0.2954756 , -0.0481694 , -0.5923031 , ..., -0.14764455,\n",
       "        -1.13673365, -0.04468802],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokener_dict=tokenizer.word_index\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samso\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix=np.zeros((NUM,200))\n",
    "\n",
    "for word,i in tokener_dict.items():\n",
    "    try:\n",
    "        vector=model[word]\n",
    "        embedding_matrix[i]=vector\n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание и обучение сверточной нейросети , с помощью библиотек Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM=200\n",
    "\n",
    "comment_input = Input(shape=(SENTENCE_LENGTH,), dtype='int32')\n",
    "comment_encoder = Embedding(NUM, DIM, input_length=SENTENCE_LENGTH,\n",
    "                          weights=[embedding_matrix], trainable=False)(comment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = []\n",
    "x = Dropout(0.2)(comment_encoder)\n",
    "\n",
    "for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10)]:\n",
    "    for i in range(filters_count):\n",
    "        # Добавляем слой свертки\n",
    "        branch = Conv1D(filters=1, kernel_size=size, padding='valid', activation='relu')(x)\n",
    "        # Добавляем слой субдискретизации\n",
    "        branch = GlobalMaxPooling1D()(branch)\n",
    "        branches.append(branch)\n",
    "        \n",
    "# Конкатенируем карты признаков\n",
    "x = concatenate(branches, axis=1)\n",
    "# Добавляем dropout-регуляризацию\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(30, activation='relu')(x)\n",
    "x = Dense(30, activation ='relu')(x)\n",
    "x = Dense(1)(x)\n",
    "output = Activation('relu')(x)\n",
    "\n",
    "model = Model(inputs=[comment_input], outputs=[output]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score, f1_score\n",
    "\n",
    "def precision(y_true,y_pred):\n",
    "    return precision_score(y_true,y_pred)\n",
    "\n",
    "def recall(y_true,y_pred):\n",
    "    return recall_score(y_true,y_pred)\n",
    "\n",
    "def f1(y_true,y_pred):\n",
    "    return f1_score(y_true,y_pred)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49994 samples, validate on 16665 samples\n",
      "Epoch 1/5\n",
      "49994/49994 [==============================] - 346s 7ms/step - loss: 0.8111 - acc: 0.5608 - val_loss: 0.6466 - val_acc: 0.6217\n",
      "Epoch 2/5\n",
      "49994/49994 [==============================] - 351s 7ms/step - loss: 0.6727 - acc: 0.6281 - val_loss: 0.5959 - val_acc: 0.6838\n",
      "Epoch 3/5\n",
      "49994/49994 [==============================] - 345s 7ms/step - loss: 0.6730 - acc: 0.6375 - val_loss: 0.6870 - val_acc: 0.5609\n",
      "Epoch 4/5\n",
      "49994/49994 [==============================] - 345s 7ms/step - loss: 0.6521 - acc: 0.6296 - val_loss: 0.5815 - val_acc: 0.7014\n",
      "Epoch 5/5\n",
      "49994/49994 [==============================] - 432s 9ms/step - loss: 0.6274 - acc: 0.6685 - val_loss: 0.5876 - val_acc: 0.6947\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_seq, y_train, batch_size=40, epochs=5, validation_split=0.25,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33330/33330 [==============================] - 81s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6948394839412416"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(x_test_seq, y_test, batch_size=32)\n",
    "score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили примерно 70% успеха ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
